{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torchsde.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyRQSZfS1zdukVWEEmFn1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farenga/neuralODE/blob/main/latent_sde_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsde"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9dOPiWaCW0B",
        "outputId": "164a1b21-04b3-442a-faf5-0786e361ac79"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsde in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: numpy>=1.19.* in /usr/local/lib/python3.7/dist-packages (from torchsde) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from torchsde) (1.10.0+cu111)\n",
            "Requirement already satisfied: boltons>=20.2.1 in /usr/local/lib/python3.7/dist-packages (from torchsde) (21.0.0)\n",
            "Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from torchsde) (0.1.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from torchsde) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->torchsde) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_gpu = False\n",
        "debug = False\n",
        "seed = 0\n",
        "train_dir = 'train'\n",
        "save_ckpt = False\n",
        "data = 'segmented_cosine'\n",
        "kl_anneal_iters = 100\n",
        "train_iters = 50\n",
        "pause_iters = 5\n",
        "batch_size = 512\n",
        "likelihood = 'laplace'\n",
        "scale = 0.05\n",
        "adjoint = False\n",
        "adaptive = False\n",
        "method = 'euler'\n",
        "dt = 1e-2\n",
        "rtol = 1e-3\n",
        "atol = 1e-3\n",
        "show_prior = True\n",
        "show_samples = True\n",
        "show_percentiles = True\n",
        "show_arrows = True\n",
        "show_mean = False\n",
        "hide_ticks = False\n",
        "dpi = 300\n",
        "color = 'blue'"
      ],
      "metadata": {
        "id": "nUZ3lYjcEybO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Latent SDE fit to a single time series with uncertainty quantification.\"\"\"\n",
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from typing import Optional, Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "from torch import distributions, nn, optim\n",
        "\n",
        "import torchsde\n",
        "\n",
        "# w/ underscore -> numpy; w/o underscore -> torch.\n",
        "Data = namedtuple('Data', ['ts_', 'ts_ext_', 'ts_vis_', 'ts', 'ts_ext', 'ts_vis', 'ys', 'ys_'])\n",
        "\n",
        "\n",
        "class LinearScheduler(object):\n",
        "    def __init__(self, iters, maxval=1.0):\n",
        "        self._iters = max(1, iters)\n",
        "        self._val = maxval / self._iters\n",
        "        self._maxval = maxval\n",
        "\n",
        "    def step(self):\n",
        "        self._val = min(self._maxval, self._val + self._maxval / self._iters)\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self._val\n",
        "\n",
        "\n",
        "class EMAMetric(object):\n",
        "    def __init__(self, gamma: Optional[float] = .99):\n",
        "        super(EMAMetric, self).__init__()\n",
        "        self._val = 0.\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def step(self, x: Union[torch.Tensor, np.ndarray]):\n",
        "        x = x.detach().cpu().numpy() if torch.is_tensor(x) else x\n",
        "        self._val = self._gamma * self._val + (1 - self._gamma) * x\n",
        "        return self._val\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self._val\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    \"\"\"Used for boolean arguments in argparse; avoiding `store_true` and `store_false`.\"\"\"\n",
        "    if isinstance(v, bool): return v\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'): return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'): return False\n",
        "    else: raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "\n",
        "def manual_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "def _stable_division(a, b, epsilon=1e-7):\n",
        "    b = torch.where(b.abs().detach() > epsilon, b, torch.full_like(b, fill_value=epsilon) * b.sign())\n",
        "    return a / b\n",
        "\n",
        "\n",
        "class LatentSDE(torchsde.SDEIto):\n",
        "\n",
        "    def __init__(self, theta=1.0, mu=0.0, sigma=0.5):\n",
        "        super(LatentSDE, self).__init__(noise_type=\"diagonal\")\n",
        "        logvar = math.log(sigma ** 2 / (2. * theta))\n",
        "\n",
        "        # Prior drift.\n",
        "        self.register_buffer(\"theta\", torch.tensor([[theta]]))\n",
        "        self.register_buffer(\"mu\", torch.tensor([[mu]]))\n",
        "        self.register_buffer(\"sigma\", torch.tensor([[sigma]]))\n",
        "\n",
        "        # p(y0).\n",
        "        self.register_buffer(\"py0_mean\", torch.tensor([[mu]]))\n",
        "        self.register_buffer(\"py0_logvar\", torch.tensor([[logvar]]))\n",
        "\n",
        "        # Approximate posterior drift: Takes in 2 positional encodings and the state.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3, 200),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(200, 200),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(200, 1)\n",
        "        )\n",
        "        # Initialization trick from Glow.\n",
        "        self.net[-1].weight.data.fill_(0.)\n",
        "        self.net[-1].bias.data.fill_(0.)\n",
        "\n",
        "        # q(y0).\n",
        "        self.qy0_mean = nn.Parameter(torch.tensor([[mu]]), requires_grad=True)\n",
        "        self.qy0_logvar = nn.Parameter(torch.tensor([[logvar]]), requires_grad=True)\n",
        "\n",
        "    def f(self, t, y):  # Approximate posterior drift.\n",
        "        if t.dim() == 0:\n",
        "            t = torch.full_like(y, fill_value=t)\n",
        "        # Positional encoding in transformers for time-inhomogeneous posterior.\n",
        "        return self.net(torch.cat((torch.sin(t), torch.cos(t), y), dim=-1))\n",
        "\n",
        "    def g(self, t, y):  # Shared diffusion.\n",
        "        return self.sigma.repeat(y.size(0), 1)\n",
        "\n",
        "    def h(self, t, y):  # Prior drift.\n",
        "        return self.theta * (self.mu - y)\n",
        "\n",
        "    def f_aug(self, t, y):  # Drift for augmented dynamics with logqp term.\n",
        "        y = y[:, 0:1]\n",
        "        f, g, h = self.f(t, y), self.g(t, y), self.h(t, y)\n",
        "        u = _stable_division(f - h, g)\n",
        "        f_logqp = .5 * (u ** 2).sum(dim=1, keepdim=True)\n",
        "        return torch.cat([f, f_logqp], dim=1)\n",
        "\n",
        "    def g_aug(self, t, y):  # Diffusion for augmented dynamics with logqp term.\n",
        "        y = y[:, 0:1]\n",
        "        g = self.g(t, y)\n",
        "        g_logqp = torch.zeros_like(y)\n",
        "        return torch.cat([g, g_logqp], dim=1)\n",
        "\n",
        "    def forward(self, ts, batch_size, eps=None):\n",
        "        eps = torch.randn(batch_size, 1).to(self.qy0_std) if eps is None else eps\n",
        "        y0 = self.qy0_mean + eps * self.qy0_std\n",
        "        qy0 = distributions.Normal(loc=self.qy0_mean, scale=self.qy0_std)\n",
        "        py0 = distributions.Normal(loc=self.py0_mean, scale=self.py0_std)\n",
        "        logqp0 = distributions.kl_divergence(qy0, py0).sum(dim=1)  # KL(t=0).\n",
        "\n",
        "        aug_y0 = torch.cat([y0, torch.zeros(batch_size, 1).to(y0)], dim=1)\n",
        "        aug_ys = sdeint_fn(\n",
        "            sde=self,\n",
        "            y0=aug_y0,\n",
        "            ts=ts,\n",
        "            method=method,\n",
        "            dt=dt,\n",
        "            adaptive=adaptive,\n",
        "            rtol=rtol,\n",
        "            atol=atol,\n",
        "            names={'drift': 'f_aug', 'diffusion': 'g_aug'}\n",
        "        )\n",
        "        ys, logqp_path = aug_ys[:, :, 0:1], aug_ys[-1, :, 1]\n",
        "        logqp = (logqp0 + logqp_path).mean(dim=0)  # KL(t=0) + KL(path).\n",
        "        return ys, logqp\n",
        "\n",
        "    def sample_p(self, ts, batch_size, eps=None, bm=None):\n",
        "        eps = torch.randn(batch_size, 1).to(self.py0_mean) if eps is None else eps\n",
        "        y0 = self.py0_mean + eps * self.py0_std\n",
        "        return sdeint_fn(self, y0, ts, bm=bm, method='srk', dt=dt, names={'drift': 'h'})\n",
        "\n",
        "    def sample_q(self, ts, batch_size, eps=None, bm=None):\n",
        "        eps = torch.randn(batch_size, 1).to(self.qy0_mean) if eps is None else eps\n",
        "        y0 = self.qy0_mean + eps * self.qy0_std\n",
        "        return sdeint_fn(self, y0, ts, bm=bm, method='srk', dt=dt)\n",
        "\n",
        "    @property\n",
        "    def py0_std(self):\n",
        "        return torch.exp(.5 * self.py0_logvar)\n",
        "\n",
        "    @property\n",
        "    def qy0_std(self):\n",
        "        return torch.exp(.5 * self.qy0_logvar)\n",
        "\n",
        "\n",
        "def make_segmented_cosine_data():\n",
        "    ts_ = np.concatenate((np.linspace(0.3, 0.8, 10), np.linspace(1.2, 1.5, 10)), axis=0)\n",
        "    ts_ext_ = np.array([0.] + list(ts_) + [2.0])\n",
        "    ts_vis_ = np.linspace(0., 2.0, 300)\n",
        "    ys_ = np.cos(ts_ * (2. * math.pi))[:, None]\n",
        "\n",
        "    ts = torch.tensor(ts_).float()\n",
        "    ts_ext = torch.tensor(ts_ext_).float()\n",
        "    ts_vis = torch.tensor(ts_vis_).float()\n",
        "    ys = torch.tensor(ys_).float().to(device)\n",
        "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)\n",
        "\n",
        "\n",
        "def make_irregular_sine_data():\n",
        "    ts_ = np.sort(np.random.uniform(low=0.4, high=1.6, size=16))\n",
        "    ts_ext_ = np.array([0.] + list(ts_) + [2.0])\n",
        "    ts_vis_ = np.linspace(0., 2.0, 300)\n",
        "    ys_ = np.sin(ts_ * (2. * math.pi))[:, None] * 0.8\n",
        "\n",
        "    ts = torch.tensor(ts_).float()\n",
        "    ts_ext = torch.tensor(ts_ext_).float()\n",
        "    ts_vis = torch.tensor(ts_vis_).float()\n",
        "    ys = torch.tensor(ys_).float().to(device)\n",
        "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)\n",
        "\n",
        "\n",
        "def make_data():\n",
        "    data_constructor = {\n",
        "        'segmented_cosine': make_segmented_cosine_data,\n",
        "        'irregular_sine': make_irregular_sine_data\n",
        "    }[data]\n",
        "    return data_constructor()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Dataset.\n",
        "    ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_ = make_data()\n",
        "\n",
        "    # Plotting parameters.\n",
        "    vis_batch_size = 1024\n",
        "    ylims = (-1.75, 1.75)\n",
        "    alphas = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55]\n",
        "    percentiles = [0.999, 0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "    vis_idx = np.random.permutation(vis_batch_size)\n",
        "    # From https://colorbrewer2.org/.\n",
        "    if color == \"blue\":\n",
        "        sample_colors = ('#8c96c6', '#8c6bb1', '#810f7c')\n",
        "        fill_color = '#9ebcda'\n",
        "        mean_color = '#4d004b'\n",
        "        num_samples = len(sample_colors)\n",
        "    else:\n",
        "        sample_colors = ('#fc4e2a', '#e31a1c', '#bd0026')\n",
        "        fill_color = '#fd8d3c'\n",
        "        mean_color = '#800026'\n",
        "        num_samples = len(sample_colors)\n",
        "\n",
        "    eps = torch.randn(vis_batch_size, 1).to(device)  # Fix seed for the random draws used in the plots.\n",
        "    bm = torchsde.BrownianInterval(\n",
        "        t0=ts_vis[0],\n",
        "        t1=ts_vis[-1],\n",
        "        size=(vis_batch_size, 1),\n",
        "        device=device,\n",
        "        levy_area_approximation='space-time'\n",
        "    )  # We need space-time Levy area to use the SRK solver\n",
        "\n",
        "    # Model.\n",
        "    model = LatentSDE().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=.999)\n",
        "    kl_scheduler = LinearScheduler(iters=kl_anneal_iters)\n",
        "\n",
        "    logpy_metric = EMAMetric()\n",
        "    kl_metric = EMAMetric()\n",
        "    loss_metric = EMAMetric()\n",
        "\n",
        "    if show_prior:\n",
        "        with torch.no_grad():\n",
        "            zs = model.sample_p(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze()\n",
        "            ts_vis_, zs_ = ts_vis.cpu().numpy(), zs.cpu().numpy()\n",
        "            zs_ = np.sort(zs_, axis=1)\n",
        "\n",
        "            img_dir = os.path.join(train_dir, 'prior.png')\n",
        "            plt.subplot(frameon=False)\n",
        "            for alpha, percentile in zip(alphas, percentiles):\n",
        "                idx = int((1 - percentile) / 2. * vis_batch_size)\n",
        "                zs_bot_ = zs_[:, idx]\n",
        "                zs_top_ = zs_[:, -idx]\n",
        "                plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
        "\n",
        "            # `zorder` determines who's on top; the larger the more at the top.\n",
        "            plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # Data.\n",
        "            plt.ylim(ylims)\n",
        "            plt.xlabel('$t$')\n",
        "            plt.ylabel('$Y_t$')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(img_dir, dpi=dpi)\n",
        "            plt.close()\n",
        "            logging.info(f'Saved prior figure at: {img_dir}')\n",
        "\n",
        "    for global_step in tqdm.tqdm(range(train_iters)):\n",
        "        # Plot and save.\n",
        "        if global_step % pause_iters == 0:\n",
        "            img_path = os.path.join(train_dir, f'global_step_{global_step}.png')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                zs = model.sample_q(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze()\n",
        "                samples = zs[:, vis_idx]\n",
        "                ts_vis_, zs_, samples_ = ts_vis.cpu().numpy(), zs.cpu().numpy(), samples.cpu().numpy()\n",
        "                zs_ = np.sort(zs_, axis=1)\n",
        "                plt.subplot(frameon=False)\n",
        "\n",
        "                if show_percentiles:\n",
        "                    for alpha, percentile in zip(alphas, percentiles):\n",
        "                        idx = int((1 - percentile) / 2. * vis_batch_size)\n",
        "                        zs_bot_, zs_top_ = zs_[:, idx], zs_[:, -idx]\n",
        "                        plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
        "\n",
        "                if show_mean:\n",
        "                    plt.plot(ts_vis_, zs_.mean(axis=1), color=mean_color)\n",
        "\n",
        "                if show_samples:\n",
        "                    for j in range(num_samples):\n",
        "                        plt.plot(ts_vis_, samples_[:, j], color=sample_colors[j], linewidth=1.0)\n",
        "\n",
        "                if show_arrows:\n",
        "                    num, dt = 12, 0.12\n",
        "                    t, y = torch.meshgrid(\n",
        "                        [torch.linspace(0.2, 1.8, num).to(device), torch.linspace(-1.5, 1.5, num).to(device)]\n",
        "                    )\n",
        "                    t, y = t.reshape(-1, 1), y.reshape(-1, 1)\n",
        "                    fty = model.f(t=t, y=y).reshape(num, num)\n",
        "                    dt = torch.zeros(num, num).fill_(dt).to(device)\n",
        "                    dy = fty * dt\n",
        "                    dt_, dy_, t_, y_ = dt.cpu().numpy(), dy.cpu().numpy(), t.cpu().numpy(), y.cpu().numpy()\n",
        "                    plt.quiver(t_, y_, dt_, dy_, alpha=0.3, edgecolors='k', width=0.0035, scale=50)\n",
        "\n",
        "                if hide_ticks:\n",
        "                    plt.xticks([], [])\n",
        "                    plt.yticks([], [])\n",
        "\n",
        "                plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # Data.\n",
        "                plt.ylim(ylims)\n",
        "                plt.xlabel('$t$')\n",
        "                plt.ylabel('$Y_t$')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(img_path, dpi=dpi)\n",
        "                plt.close()\n",
        "                logging.info(f'Saved figure at: {img_path}')\n",
        "\n",
        "                if save_ckpt:\n",
        "                    torch.save(\n",
        "                        {'model': model.state_dict(),\n",
        "                         'optimizer': optimizer.state_dict(),\n",
        "                         'scheduler': scheduler.state_dict(),\n",
        "                         'kl_scheduler': kl_scheduler},\n",
        "                        os.path.join(ckpt_dir, f'global_step_{global_step}.ckpt')\n",
        "                    )\n",
        "\n",
        "        # Train.\n",
        "        optimizer.zero_grad()\n",
        "        zs, kl = model(ts=ts_ext, batch_size=batch_size)\n",
        "        zs = zs.squeeze()\n",
        "        zs = zs[1:-1]  # Drop first and last which are only used to penalize out-of-data region and spread uncertainty.\n",
        "\n",
        "        likelihood_constructor = distributions.Laplace\n",
        "        likelihood = likelihood_constructor(loc=zs, scale=scale)\n",
        "        logpy = likelihood.log_prob(ys).sum(dim=0).mean(dim=0)\n",
        "\n",
        "        loss = -logpy + kl * kl_scheduler.val\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        kl_scheduler.step()\n",
        "\n",
        "        logpy_metric.step(logpy)\n",
        "        kl_metric.step(kl)\n",
        "        loss_metric.step(loss)\n",
        "\n",
        "        logging.info(\n",
        "            f'global_step: {global_step}, '\n",
        "            f'logpy: {logpy_metric.val:.3f}, '\n",
        "            f'kl: {kl_metric.val:.3f}, '\n",
        "            f'loss: {loss_metric.val:.3f}'\n",
        "        )\n",
        "\n",
        "\n",
        "# The argparse format supports both `--boolean-argument` and `--boolean-argument True`.\n",
        "# Trick from https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse.\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and not no_gpu else 'cpu')\n",
        "manual_seed(seed)\n",
        "\n",
        "if debug:\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "ckpt_dir = os.path.join(train_dir, 'ckpts')\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "sdeint_fn = torchsde.sdeint_adjoint if adjoint else torchsde.sdeint\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by1mfmuBC1Ct",
        "outputId": "89a196c6-10b6-4c6c-f51d-409d6930d2bf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [02:19<00:00,  2.80s/it]\n"
          ]
        }
      ]
    }
  ]
}