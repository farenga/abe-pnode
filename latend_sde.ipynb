{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "latend_sde.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMmPuT+tO5OCG9JHG6U8Ccs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farenga/neuralODE/blob/main/latend_sde.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsde"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3M3niuAxPo4",
        "outputId": "4afc3d4e-82ef-4228-fcba-f1061f60d0b9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsde in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: boltons>=20.2.1 in /usr/local/lib/python3.7/dist-packages (from torchsde) (21.0.0)\n",
            "Requirement already satisfied: numpy>=1.19.* in /usr/local/lib/python3.7/dist-packages (from torchsde) (1.19.5)\n",
            "Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from torchsde) (0.1.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from torchsde) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from torchsde) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->torchsde) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from typing import Optional, Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchsde\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import distributions, nn, optim\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "qeA1OB5XyKj0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QeBrusWJvWp4"
      },
      "outputs": [],
      "source": [
        "# w/ underscore -> numpy; w/o underscore -> torch.\n",
        "\n",
        "Data = namedtuple('Data', ['ts_', 'ts_ext_', 'ts_vis_', 'ts', 'ts_ext', 'ts_vis', 'ys', 'ys_'])\n",
        "POOL_SIZE = 16\n",
        "CACHE_SIZE = 500\n",
        "\n",
        "\n",
        "def write_config(args: argparse.Namespace):\n",
        "    os.makedirs(_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(train_dir, 'ckpts'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(train_dir, 'plots'), exist_ok=True)\n",
        "    config_path = os.path.join(train_dir, 'config.json')\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(__dict__, f, indent=4)\n",
        "    logging.warning(f\"Wrote config: {config_path}\")\n",
        "\n",
        "\n",
        "class ConstantScheduler(object):\n",
        "    def __init__(self, constant=1.0):\n",
        "        self._constant = constant\n",
        "        self._val = constant\n",
        "\n",
        "    def step(self, current_itr):\n",
        "        self._val = self._constant\n",
        "\n",
        "    def set(self, x):\n",
        "        self._val = x\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self._val\n",
        "\n",
        "\n",
        "class LinearScheduler(object):\n",
        "    def __init__(self, iters, maxval=1.0):\n",
        "        self._iters = max(1, iters)\n",
        "        self._val = maxval / self._iters\n",
        "        self._maxval = maxval\n",
        "\n",
        "    def step(self):\n",
        "        self._val = min(self._maxval, self._val + self._maxval / self._iters)\n",
        "\n",
        "    def set(self, x):\n",
        "        self._val = x\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self._val\n",
        "\n",
        "\n",
        "class LinearDecayScheduler(object):\n",
        "    def __init__(self, iters, maxval=100.0):\n",
        "        self._iters = max(1, iters)\n",
        "        self._val = maxval\n",
        "        self._minval = maxval / self._iters\n",
        "\n",
        "    def step(self):\n",
        "        self._val = max(self._minval, self._val - self._minval)\n",
        "\n",
        "    def set(self, x):\n",
        "        self._val = x\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self._val\n",
        "\n",
        "\n",
        "class HalfwayScheduler(object):\n",
        "    def __init__(self, iters=5000, maxval=1.0, halfway_val=100.):\n",
        "        self._iters = max(1, iters)\n",
        "        self._val = maxval\n",
        "        self._maxval = maxval\n",
        "        self._halfway_val = halfway_val\n",
        "\n",
        "    def step(self, current_itr):\n",
        "        halfway = self._iters // 2\n",
        "        self._val = self._maxval if current_itr < halfway else self._halfway_val\n",
        "\n",
        "    def set(self, x):\n",
        "        self._val = x\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self._val\n",
        "\n",
        "\n",
        "class EMAMetric(object):\n",
        "    def __init__(self, gamma: Optional[float] = .99):\n",
        "        super(EMAMetric, self).__init__()\n",
        "        self._val = 0.\n",
        "        self._gamma = gamma\n",
        "\n",
        "    def step(self, x: Union[torch.Tensor, np.ndarray]):\n",
        "        x = x.detach().cpu().numpy() if torch.is_tensor(x) else x\n",
        "        self._val = self._gamma * self._val + (1 - self._gamma) * x\n",
        "        return self._val\n",
        "\n",
        "    def set(self, x: Union[torch.Tensor, np.ndarray]):\n",
        "        self._val = x\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self._val\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    \"\"\"Used for boolean arguments in argparse; avoiding `store_true` and `store_false`.\"\"\"\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "\n",
        "def make_optimizer(optimizer, params, lr):\n",
        "    optimizer_constructor = {\n",
        "        \"adam\": optim.Adam,\n",
        "        \"adamax\": optim.Adamax,\n",
        "        \"adadelta\": optim.Adadelta,\n",
        "        \"adagrad\": optim.Adagrad,\n",
        "        \"sgd\": optim.SGD\n",
        "    }[optimizer]\n",
        "    return optimizer_constructor(params=params, lr=lr)\n",
        "\n",
        "\n",
        "def manual_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "def _stable_division(a, b, epsilon=1e-7):\n",
        "    b = torch.where(b.abs().detach() > epsilon, b, torch.full_like(b, fill_value=epsilon) * b.sign())\n",
        "    return a / b\n",
        "\n",
        "\n",
        "class LatentSDE(torchsde.SDEIto):\n",
        "\n",
        "    def __init__(self, theta=1.0, mu=0.0, sigma=0.5):\n",
        "        super(LatentSDE, self).__init__(noise_type=\"diagonal\")\n",
        "        logvar = math.log(sigma ** 2 / (2. * theta))\n",
        "\n",
        "        # Prior drift.\n",
        "        self.register_buffer(\"theta\", torch.tensor([[theta]]))\n",
        "        self.register_buffer(\"mu\", torch.tensor([[mu]]))\n",
        "        self.register_buffer(\"sigma\", torch.tensor([[sigma]]))\n",
        "\n",
        "        # p(y0).\n",
        "        self.register_buffer(\"py0_mean\", torch.tensor([[mu]]))\n",
        "        self.register_buffer(\"py0_logvar\", torch.tensor([[logvar]]))\n",
        "\n",
        "        # Approximate posterior drift: Takes in 2 positional encodings and the state.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2, 200),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(200, 200),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(200, 1)\n",
        "        )\n",
        "        # Initialization trick from Glow.\n",
        "        self.net[-1].weight.data.fill_(0.)\n",
        "        self.net[-1].bias.data.fill_(0.)\n",
        "\n",
        "        # q(y0).\n",
        "        self.qy0_mean = nn.Parameter(torch.tensor([[mu]]), requires_grad=False)\n",
        "        self.qy0_logvar = nn.Parameter(torch.tensor([[logvar]]), requires_grad=False)\n",
        "\n",
        "    def f(self, t, y):  # Approximate posterior drift.\n",
        "        return self.net(torch.cat((t.expand_as(y[:, :1]), y), dim=-1))\n",
        "\n",
        "    def g(self, t, y):  # Shared diffusion.\n",
        "        return self.sigma.repeat(y.size(0), 1)\n",
        "\n",
        "    def h(self, t, y):  # Prior drift.\n",
        "        return self.theta * (self.mu - y)\n",
        "\n",
        "    def f_aug(self, t, y):  # Drift for augmented dynamics with logqp term.\n",
        "        y = y[:, 0:1]\n",
        "        f, g, h = self.f(t, y), self.g(t, y), self.h(t, y)\n",
        "        u = _stable_division(f - h, g)\n",
        "        f_logqp = .5 * (u ** 2).sum(dim=1, keepdim=True)\n",
        "        return torch.cat([f, f_logqp], dim=1)\n",
        "\n",
        "    def g_aug(self, t, y):  # Diffusion for augmented dynamics with logqp term.\n",
        "        y = y[:, 0:1]\n",
        "        g = self.g(t, y)\n",
        "        g_logqp = torch.zeros_like(y)\n",
        "        return torch.cat([g, g_logqp], dim=1)\n",
        "\n",
        "    def forward(self, ts, batch_size, eps=None):\n",
        "        eps = torch.randn(batch_size, 1).to(self.qy0_std) if eps is None else eps\n",
        "        y0 = self.qy0_mean + eps * self.qy0_std\n",
        "        qy0 = distributions.Normal(loc=self.qy0_mean, scale=self.qy0_std)\n",
        "        py0 = distributions.Normal(loc=self.py0_mean, scale=self.py0_std)\n",
        "        logqp0 = distributions.kl_divergence(qy0, py0).sum(dim=1)  # KL(t=0).\n",
        "\n",
        "        bm = torchsde.BrownianInterval(\n",
        "            t0=ts[0],\n",
        "            t1=ts[-1],\n",
        "            dtype=y0.dtype,\n",
        "            device=y0.device,\n",
        "            size=(batch_size, 2),\n",
        "            pool_size=POOL_SIZE,\n",
        "            cache_size=CACHE_SIZE\n",
        "        )\n",
        "        aug_y0 = torch.cat([y0, torch.zeros(batch_size, 1).to(y0)], dim=1)\n",
        "        aug_ys = sdeint_fn(\n",
        "            sde=self,\n",
        "            bm=bm,\n",
        "            y0=aug_y0,\n",
        "            ts=ts.to(device),\n",
        "            method=method,\n",
        "            dt=dt,\n",
        "            adaptive=adaptive,\n",
        "            adjoint_adaptive=adjoint_adaptive,\n",
        "            rtol=rtol,\n",
        "            atol=atol,\n",
        "            names={'drift': 'f_aug', 'diffusion': 'g_aug'}\n",
        "        )\n",
        "        ys, logqp_path = aug_ys[:, :, 0:1], aug_ys[-1, :, 1]\n",
        "        logqp = (logqp0 + logqp_path).mean(dim=0)  # KL(t=0) + KL(path).\n",
        "        return ys, logqp\n",
        "\n",
        "    def sample_p(self, ts, batch_size, eps=None, bm=None):\n",
        "        eps = torch.randn(batch_size, 1).to(self.py0_mean) if eps is None else eps\n",
        "        y0 = self.py0_mean + eps * self.py0_std\n",
        "        # TODO:\n",
        "        return sdeint_fn(self, y0, ts.to(device), bm=bm, method='euler', dt=dt, names={'drift': 'h'})\n",
        "\n",
        "    def sample_q(self, ts, batch_size, eps=None, bm=None):\n",
        "        eps = torch.randn(batch_size, 1).to(self.qy0_mean) if eps is None else eps\n",
        "        y0 = self.qy0_mean + eps * self.qy0_std\n",
        "        # TODO:\n",
        "        return sdeint_fn(self, y0, ts.to(device), bm=bm, method='euler', dt=dt)\n",
        "\n",
        "    @property\n",
        "    def py0_std(self):\n",
        "        return torch.exp(.5 * self.py0_logvar)\n",
        "\n",
        "    @property\n",
        "    def qy0_std(self):\n",
        "        return torch.exp(.5 * self.qy0_logvar)\n",
        "\n",
        "\n",
        "def make_segmented_cosine_data():\n",
        "    ts0_, ts1_ = np.linspace(0.3, 0.6, 2)[:0], np.linspace(1.40, 1.5, 1)[:0]\n",
        "    ys0_, ys1_ = tuple(np.cos(t * (2. * math.pi)) for t in (ts0_, ts1_))\n",
        "    ts_ = np.concatenate((ts0_, np.array([1.0]), ts1_))\n",
        "    ts_ext_ = np.array([0.4] + list(ts_) + [1.6])\n",
        "    ts_vis_ = np.linspace(0.4, 1.6, 300)\n",
        "    ys_ = np.stack(\n",
        "        (\n",
        "            np.concatenate((ys0_, np.array([-1.41]), ys1_)),\n",
        "            np.concatenate((ys0_, np.array([-0.21]), ys1_)),\n",
        "        ), axis=1\n",
        "    )\n",
        "\n",
        "    ts = torch.tensor(ts_).float()\n",
        "    ts_ext = torch.tensor(ts_ext_).float()\n",
        "    ts_vis = torch.tensor(ts_vis_).float()\n",
        "    ys = torch.tensor(ys_).float().to(device)\n",
        "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)\n",
        "\n",
        "\n",
        "def make_irregular_sine_data():\n",
        "    ts_ = np.sort(np.random.uniform(low=0.4, high=1.6, size=16))\n",
        "    ts_ext_ = np.array([0.] + list(ts_) + [2.0])\n",
        "    ts_vis_ = np.linspace(0., 2.0, 300)\n",
        "    ys_ = np.sin(ts_ * (2. * math.pi))[:, None] * 0.8\n",
        "\n",
        "    ts = torch.tensor(ts_).float()\n",
        "    ts_ext = torch.tensor(ts_ext_).float()\n",
        "    ts_vis = torch.tensor(ts_vis_).float()\n",
        "    ys = torch.tensor(ys_).float().to(device)\n",
        "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)\n",
        "\n",
        "\n",
        "def make_data():\n",
        "    data_constructor = {\n",
        "        'segmented_cosine': make_segmented_cosine_data,\n",
        "        'irregular_sine': make_irregular_sine_data\n",
        "    }[data]\n",
        "    return data_constructor()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Dataset.\n",
        "    ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_ = make_data()\n",
        "    summary = SummaryWriter(os.path.join(train_dir, 'tb'))\n",
        "\n",
        "    # Plotting parameters.\n",
        "    vis_batch_size = 1024\n",
        "    ylims = (-1.75, 1.75)\n",
        "    alphas = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55]\n",
        "    percentiles = [0.999, 0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "    vis_idx = np.random.permutation(vis_batch_size)\n",
        "    if color == \"blue\":\n",
        "        fill_color = '#9ebcda'\n",
        "        mean_color = '#4d004b'\n",
        "        num_samples = 60\n",
        "    else:\n",
        "        sample_colors = ('#fc4e2a', '#e31a1c', '#bd0026')\n",
        "        fill_color = '#fd8d3c'\n",
        "        mean_color = '#800026'\n",
        "        num_samples = len(sample_colors)\n",
        "\n",
        "    eps = torch.randn(vis_batch_size, 1).to(device)  # Fix seed for the random draws used in the plots.\n",
        "    bm = torchsde.BrownianInterval(\n",
        "        t0=ts_vis[0],\n",
        "        t1=ts_vis[-1],\n",
        "        size=(vis_batch_size, 1),\n",
        "        device=device,\n",
        "        levy_area_approximation='space-time',\n",
        "        pool_size=POOL_SIZE,\n",
        "        cache_size=CACHE_SIZE,\n",
        "    )  # We need space-time Levy area to use the SRK solver\n",
        "\n",
        "    # Model.\n",
        "    # Note: This `mu` is selected based on the yvalue of the two endpoints of the left and right segments.\n",
        "    model = LatentSDE(mu=-0.80901699, sigma=sigma).to(device)\n",
        "    optimizer = make_optimizer(optimizer=optimizer_type, params=model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=.99997)\n",
        "    kl_scheduler = LinearScheduler(iters=kl_anneal_iters, maxval=kl_coeff)\n",
        "    nll_scheduler = ConstantScheduler(constant=nll_coef)\n",
        "\n",
        "    logpy_metric = EMAMetric()\n",
        "    kl_metric = EMAMetric()\n",
        "    loss_metric = EMAMetric()\n",
        "\n",
        "    if os.path.exists(os.path.join(train_dir, 'ckpts', f'state.ckpt')):\n",
        "        logging.info(\"Loading checkpoints...\")\n",
        "        checkpoint = torch.load(os.path.join(train_dir, 'ckpts', f'state.ckpt'))\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "        try:\n",
        "            logpy_metric.set(checkpoint['logpy_metric'])\n",
        "            kl_metric.set(checkpoint['kl_metric'])\n",
        "            loss_metric.set(checkpoint['loss_metric'])\n",
        "        except:\n",
        "            logging.warning(f\"Could not successfully load logpy, kl, and loss metrics from checkpoint\")\n",
        "        logging.info(f\"Successfully loaded checkpoints at global_step {checkpoint['global_step']}\")\n",
        "\n",
        "    if show_prior:\n",
        "        with torch.no_grad():\n",
        "            zs = model.sample_p(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze()\n",
        "            ts_vis_, zs_ = ts_vis.cpu().numpy(), zs.cpu().numpy()\n",
        "            zs_ = np.sort(zs_, axis=1)\n",
        "\n",
        "            img_dir = os.path.join(train_dir, 'prior.png')\n",
        "            \n",
        "            plt.subplot(frameon=False)\n",
        "            for alpha, percentile in zip(alphas, percentiles):\n",
        "                idx = int((1 - percentile) / 2. * vis_batch_size)\n",
        "                zs_bot_ = zs_[:, idx]\n",
        "                zs_top_ = zs_[:, -idx]\n",
        "                plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
        "\n",
        "            # `zorder` determines who's on top; the larger the more at the top.\n",
        "            plt.scatter(ts_, ys_[:, 0], marker='x', zorder=3, color='k', s=35)  # Data.\n",
        "            if data != \"irregular_sine\":\n",
        "                plt.scatter(ts_, ys_[:, 1], marker='x', zorder=3, color='k', s=35)  # Data.\n",
        "            plt.ylim(ylims)\n",
        "            plt.xlabel('$t$')\n",
        "            plt.ylabel('$Y_t$')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(img_dir, dpi=dpi)\n",
        "            summary.add_figure('Prior', plt.gcf(), 0)\n",
        "            logging.info(f'Prior saved to tensorboard')\n",
        "            plt.close()\n",
        "            logging.info(f'Saved prior figure at: {img_dir}')\n",
        "\n",
        "    for global_step in tqdm.tqdm(range(train_iters)):\n",
        "        # Plot and save.\n",
        "        if global_step % pause_iters == 0 or global_step == (train_iters - 1):\n",
        "            img_path = os.path.join(train_dir, \"plots\", f'global_step_{global_step}.png')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # TODO:\n",
        "                zs = model.sample_q(ts=ts_vis, batch_size=vis_batch_size, eps=None, bm=bm).squeeze()\n",
        "                samples = zs[:, vis_idx]\n",
        "                ts_vis_, zs_, samples_ = ts_vis.cpu().numpy(), zs.cpu().numpy(), samples.cpu().numpy()\n",
        "                zs_ = np.sort(zs_, axis=1)\n",
        "                plt.subplot(frameon=False)\n",
        "\n",
        "                if show_percentiles:\n",
        "                    for alpha, percentile in zip(alphas, percentiles):\n",
        "                        idx = int((1 - percentile) / 2. * vis_batch_size)\n",
        "                        zs_bot_, zs_top_ = zs_[:, idx], zs_[:, -idx]\n",
        "                        plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
        "\n",
        "                if show_mean:\n",
        "                    plt.plot(ts_vis_, zs_.mean(axis=1), color=mean_color)\n",
        "\n",
        "                if show_samples:\n",
        "                    for j in range(num_samples):\n",
        "                        plt.plot(ts_vis_, samples_[:, j], linewidth=1.0)\n",
        "\n",
        "                if show_arrows:\n",
        "                    t_start, t_end = ts_vis_[0], ts_vis_[-1]\n",
        "                    num, dt = 12, 0.12\n",
        "                    t, y = torch.meshgrid(\n",
        "                        [torch.linspace(t_start, t_end, num).to(device), torch.linspace(*ylims, num).to(device)]\n",
        "                    )\n",
        "                    t, y = t.reshape(-1, 1), y.reshape(-1, 1)\n",
        "                    fty = model.f(t=t, y=y).reshape(num, num)\n",
        "                    dt = torch.zeros(num, num).fill_(dt).to(device)\n",
        "                    dy = fty * dt\n",
        "                    dt_, dy_, t_, y_ = dt.cpu().numpy(), dy.cpu().numpy(), t.cpu().numpy(), y.cpu().numpy()\n",
        "                    plt.quiver(t_, y_, dt_, dy_, alpha=0.3, edgecolors='k', width=0.0035, scale=50)\n",
        "\n",
        "                if hide_ticks:\n",
        "                    plt.xticks([], [])\n",
        "                    plt.yticks([], [])\n",
        "\n",
        "                plt.scatter(ts_, ys_[:, 0], marker='x', zorder=3, color='k', s=35)  # Data.\n",
        "                if data != \"irregular_sine\":\n",
        "                    plt.scatter(ts_, ys_[:, 1], marker='x', zorder=3, color='k', s=35)  # Data.\n",
        "                plt.ylim(ylims)\n",
        "                plt.xlabel('$t$')\n",
        "                plt.ylabel('$Y_t$')\n",
        "                plt.tight_layout()\n",
        "                if global_step % save_fig == 0:\n",
        "                    plt.savefig(img_path, dpi=dpi)\n",
        "                current_fig = plt.gcf()\n",
        "                summary.add_figure('Predictions plot', current_fig, global_step)\n",
        "                logging.info(f'Predictions plot saved to tensorboard')\n",
        "                plt.close()\n",
        "                logging.info(f'Saved figure at: {img_path}')\n",
        "\n",
        "                if save_ckpt:\n",
        "                    torch.save(\n",
        "                        {'model': model.state_dict(),\n",
        "                         'optimizer': optimizer.state_dict(),\n",
        "                         'scheduler': scheduler.state_dict()},\n",
        "                        os.path.join(train_dir, 'ckpts', f'global_step_{global_step}.ckpt')\n",
        "                    )\n",
        "                    # for preemption\n",
        "                    torch.save(\n",
        "                        {'model': model.state_dict(),\n",
        "                         'optimizer': optimizer.state_dict(),\n",
        "                         'scheduler': scheduler.state_dict(),\n",
        "                         'global_step': global_step,\n",
        "                         'logpy_metric': logpy_metric.val,\n",
        "                         'kl_metric': kl_metric.val,\n",
        "                         'loss_metric': loss_metric.val},\n",
        "                        os.path.join(train_dir, 'ckpts', f'state.ckpt')\n",
        "                    )\n",
        "\n",
        "        # Train.\n",
        "        optimizer.zero_grad()\n",
        "        zs, kl = model(ts=ts_ext, batch_size=batch_size)\n",
        "        zs = zs.squeeze()\n",
        "        zs = zs[1:-1]  # Drop first and last which are only used to penalize out-of-data region and spread uncertainty.\n",
        "\n",
        "        likelihood_constructor = {\n",
        "            \"laplace\": distributions.Laplace, \"normal\": distributions.Normal, \"cauchy\": distributions.Cauchy\n",
        "        }[\"normal\"]\n",
        "        likelihood = likelihood_constructor(loc=zs, scale=scale)\n",
        "\n",
        "        # Proper summation of log-likelihoods.\n",
        "        logpy = 0.\n",
        "        ys_split = ys.split(split_size=1, dim=-1)\n",
        "        for _ys in ys_split:\n",
        "            logpy = logpy + likelihood.log_prob(_ys).sum(dim=0).mean(dim=0)\n",
        "        logpy = logpy / len(ys_split)\n",
        "\n",
        "        loss = -logpy * nll_scheduler.val + kl * kl_scheduler.val\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        kl_scheduler.step()\n",
        "        nll_scheduler.step(global_step)\n",
        "\n",
        "        logpy_metric.step(logpy)\n",
        "        kl_metric.step(kl)\n",
        "        loss_metric.step(loss)\n",
        "\n",
        "        logging.info(\n",
        "            f'global_step: {global_step}, '\n",
        "            f'logpy: {logpy_metric.val:.3f}, '\n",
        "            f'kl: {kl_metric.val:.3f}, '\n",
        "            f'loss: {loss_metric.val:.3f}'\n",
        "        )\n",
        "        summary.add_scalar('KL Schedler', kl_scheduler.val, global_step)\n",
        "        summary.add_scalar('NLL Schedler', nll_scheduler.val, global_step)\n",
        "        summary.add_scalar('Loss', loss_metric.val, global_step)\n",
        "        summary.add_scalar('KL', kl_metric.val, global_step)\n",
        "        summary.add_scalar('Log(py) Likelihood', logpy_metric.val, global_step)\n",
        "        logging.info(f'Logged loss, kl, logpy to tensorboard')\n",
        "\n",
        "    summary.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_gpu = False\n",
        "debug = False\n",
        "seed = 0\n",
        "train_dir = \"train\"\n",
        "save_ckpt = False\n",
        "\n",
        "lr = 1e-3\n",
        "optimizer_type = \"adam\"\n",
        "data = 'segmented_cosine'\n",
        "kl_anneal_iters = 100\n",
        "kl_coeff = 1.\n",
        "nll_coef = 1.\n",
        "nll_decay_coef = 100.\n",
        "train_iters = 50\n",
        "pause_iters = 5\n",
        "save_fig = 5\n",
        "batch_size = 512\n",
        "likelihood = \"normal\"\n",
        "scale = .05\n",
        "sigma = .5\n",
        "\n",
        "adjoint = False\n",
        "adaptive = False\n",
        "adjoint_adaptive = False\n",
        "method = 'euler'\n",
        "dt = 1e-2\n",
        "rtol = 1e-3\n",
        "atol = 1e-3\n",
        "\n",
        "show_prior = True\n",
        "show_samples = True\n",
        "show_percentiles = False\n",
        "show_arrows = True\n",
        "show_mean = False\n",
        "hide_ticks = False\n",
        "dpi = 300\n",
        "color = 'blue'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and not no_gpu else 'cpu')\n",
        "manual_seed(seed)\n",
        "\n",
        "if debug:\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "sdeint_fn = torchsde.sdeint_adjoint if adjoint else torchsde.sdeint\n",
        "\n",
        "main()\n",
        "\n",
        "print(\"finished running :)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7GPUkJEva9T",
        "outputId": "a2932d14-db03-4f50-fd24-0601d474c97f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            " 10%|█         | 5/50 [00:04<00:36,  1.24it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            " 20%|██        | 10/50 [00:09<00:32,  1.24it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            " 30%|███       | 15/50 [00:14<00:28,  1.25it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            " 40%|████      | 20/50 [00:19<00:24,  1.24it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            " 50%|█████     | 25/50 [00:24<00:20,  1.24it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            " 60%|██████    | 30/50 [00:29<00:16,  1.24it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            " 70%|███████   | 35/50 [00:33<00:12,  1.24it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            " 80%|████████  | 40/50 [00:38<00:07,  1.25it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            " 90%|█████████ | 45/50 [00:43<00:04,  1.25it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            " 98%|█████████▊| 49/50 [00:47<00:00,  1.14it/s]/usr/local/lib/python3.7/dist-packages/torchsde/_core/misc.py:29: UserWarning: `sdeint`: Unexpected arguments {'adjoint_adaptive': False}\n",
            "  warnings.warn(f\"{msg}: Unexpected arguments {unused_kwargs}\")\n",
            "100%|██████████| 50/50 [00:49<00:00,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished running :)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}