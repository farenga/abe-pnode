{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_deep_implicit_layers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIimtzgE44SDa6VygicKM8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farenga/neuralODEs/blob/main/0_deep_implicit_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j50itZ4AVYS4"
      },
      "source": [
        "# Implicit layers\n",
        "\n",
        "When dealing with implicit layers, instead of specifying the explicit function for computing the layer's output we specify a set of conditions. By the way everything can be made implicit, like the following explict formulation:\n",
        "\\begin{equation}\n",
        "z = f(x), \\qquad f:X \\rightarrow Z \\subset \\mathbb{R}^n\n",
        "\\end{equation}\n",
        "becomes\n",
        "\\begin{equation}\n",
        "\\text{find} \\quad x \\quad \\text{s.t.} \\quad g(x,z)=0, \\qquad g(x,z)=z-f(x), \\qquad g:X\\times Z \\rightarrow \\mathbb{R}^n\n",
        "\\end{equation}\n",
        "\n",
        "### Example\n",
        "\n",
        "Let us now find $z$ such that $z = \\tanh(Wz+x)$.\n",
        "\n",
        "This means finding the root of $g(x,z)$, where $g(x,z)=z-\\tanh(Wz+x)$\n",
        "\n",
        "When we deal with implicit equations we tackle the problem with an iterative approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO6n2bzWWx51"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MJyAwxLU-Tw"
      },
      "source": [
        "max_iter = 100\n",
        "iter = 0\n",
        "tol = 1e-3\n",
        "n = 3\n",
        "z = np.zeros(shape=(n,1))\n",
        "x = np.ones(shape=(n,1))\n",
        "W = np.random.rand(n,n)\n",
        "while (iter<max_iter):\n",
        "  z_next = np.tanh(np.matmul(W,z)+x)\n",
        "  z = z_next\n",
        "  iter += 1"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAQQXrmBXmg1",
        "outputId": "72a34c4b-2857-4065-dd88-491b265a6033"
      },
      "source": [
        "z"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9974062 ],\n",
              "       [0.95144457],\n",
              "       [0.97637945]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eXjROMBYulk"
      },
      "source": [
        "Pythorch implementation via a linear layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ht8JyHTY2AF"
      },
      "source": [
        "class TanhFixedPointLayer(nn.Module):\n",
        "  def __init__(self, out_features, tol = 1e-4, max_iter = 50):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(out_features, out_features, bias = False)\n",
        "    self.tol = tol\n",
        "    self.max_iter = max_iter\n",
        "\n",
        "  def forward(self,x):\n",
        "    z = torch.zeros_like(x)\n",
        "    self.iterations = 0\n",
        "\n",
        "    while (self.iterations < self.max_iter):\n",
        "      z_next = torch.tanh(self.linear(z)+x)\n",
        "      self.err = torch.norm(z-z_next)\n",
        "      z = z_next\n",
        "      self.iterations += 1\n",
        "      if (self.err < self.tol):\n",
        "        break\n",
        "      \n",
        "    return z"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ckGoXq_Z5Kl",
        "outputId": "c5958c10-32d1-4504-bf79-5db63962939b"
      },
      "source": [
        "layer = TanhFixedPointLayer(n)\n",
        "x = torch.tensor([1.,1.,1.])\n",
        "z = layer(x)\n",
        "z\n",
        "print(z)\n",
        "print(layer.iterations)\n",
        "print(layer.err)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4906, 0.7258, 0.7385], grad_fn=<TanhBackward0>)\n",
            "13\n",
            "tensor(4.9090e-05, grad_fn=<CopyBackwards>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9EL9gZHepzx"
      },
      "source": [
        "In order to find $g(x,z)$ roots we can use Newton's method, indeed instead of performing the following fixed point iteration\n",
        "\n",
        "\\begin{equation}\n",
        "z_{n+1} = \\tanh(W z_n + x)\n",
        "\\end{equation}\n",
        "\n",
        "we can express this as an implicit equation $g(x,z)=0$ and apply Newton's method to $g$:\n",
        "\n",
        "\\begin{equation}\n",
        "z_{n+1} = z_n - \\frac{g(x,z_n)}{\\partial_z g(x,z)|_{z=z_n}}\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKeJjoECcOFV"
      },
      "source": [
        "class TanhNewtonLayer(nn.Module):\n",
        "    def __init__(self, out_features, tol = 1e-4, max_iter=50):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(out_features, out_features, bias=False)\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "  \n",
        "    def forward(self, x):\n",
        "        # initialize output z to be zero\n",
        "        z = torch.tanh(x)\n",
        "        self.iterations = 0\n",
        "    \n",
        "        # iterate until convergence\n",
        "        while self.iterations < self.max_iter:\n",
        "            z_linear = self.linear(z) + x\n",
        "            g = z - torch.tanh(z_linear)\n",
        "            self.err = torch.norm(g)\n",
        "            if self.err < self.tol:\n",
        "                break\n",
        "\n",
        "            # newton step\n",
        "            J = torch.eye(z.shape[1])[None,:,:] - (1 / torch.cosh(z_linear)**2)[:,:,None]*self.linear.weight[None,:,:]\n",
        "            z = z - torch.solve(g[:,:,None], J)[0][:,:,0]\n",
        "            self.iterations += 1\n",
        "\n",
        "        g = z - torch.tanh(self.linear(z) + x)\n",
        "        z[torch.norm(g,dim=1) > self.tol,:] = 0\n",
        "        return z"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hDkLvtSgT2_",
        "outputId": "94c31aa4-eba5-4be9-d7b8-b6942cd10c72"
      },
      "source": [
        "layer = TanhNewtonLayer(50)\n",
        "X = torch.randn(10,50)\n",
        "Z = layer(X)\n",
        "print(f\"Terminated after {layer.iterations} iterations with error {layer.err}\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terminated after 3 iterations with error 1.0844622693184647e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgd3uAWakoZG"
      },
      "source": [
        "By the way this approach is not efficient, since:\n",
        "\n",
        "\n",
        "*   At each iteration we have to compute and invert the Jacobian matrix (for each sample in the minibatch)\n",
        "*   By implementing the Newton's method directly within an automatic differentiation toolkit we have to:\n",
        "    *  Save intermediate iterates of the hidden units, that in this case means to store iterates of the Jacobian terms (High memory consumption)\n",
        "    * Backprop + Inversion can be numerically unstable\n",
        "    \n",
        "\n"
      ]
    }
  ]
}